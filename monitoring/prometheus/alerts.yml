# =============================================================================
# Plataforma E - Prometheus Alert Rules
# =============================================================================
# Issue #96: Stack de Metricas Prometheus/Grafana
#
# Regras de alerta para monitoramento da aplicacao
# =============================================================================

groups:
  # ---------------------------------------------------------------------------
  # API Alerts - Alertas da API Factory
  # ---------------------------------------------------------------------------
  - name: factory-api-alerts
    rules:
      # Alerta quando API esta down
      - alert: FactoryAPIDown
        expr: up{job="factory-api"} == 0
        for: 1m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Factory API is down"
          description: "Factory API {{ $labels.instance }} has been down for more than 1 minute."
          runbook_url: "https://docs.fabrica-agentes.com/runbooks/api-down"

      # Alerta quando latencia esta alta
      - alert: FactoryAPIHighLatency
        expr: histogram_quantile(0.95, rate(api_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High API latency detected"
          description: "95th percentile latency is {{ $value | humanizeDuration }} on {{ $labels.instance }}"

      # Alerta quando erro rate esta alto
      - alert: FactoryAPIHighErrorRate
        expr: |
          sum(rate(api_requests_total{status=~"5.."}[5m]))
          / sum(rate(api_requests_total[5m])) * 100 > 5
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High error rate on Factory API"
          description: "Error rate is {{ $value | humanize }}% over the last 5 minutes"

      # Alerta quando erro rate critico
      - alert: FactoryAPICriticalErrorRate
        expr: |
          sum(rate(api_requests_total{status=~"5.."}[5m]))
          / sum(rate(api_requests_total[5m])) * 100 > 20
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Critical error rate on Factory API"
          description: "Error rate is {{ $value | humanize }}% - Immediate action required!"

  # ---------------------------------------------------------------------------
  # Worker Alerts - Alertas dos Workers
  # ---------------------------------------------------------------------------
  - name: factory-worker-alerts
    rules:
      # Alerta quando nenhum worker esta ativo
      - alert: NoActiveWorkers
        expr: active_workers == 0
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "No active workers"
          description: "There are no active workers processing jobs. Job queue will grow indefinitely."

      # Alerta quando workers estao sobrecarregados
      - alert: WorkersOverloaded
        expr: jobs_pending / active_workers > 10
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Workers are overloaded"
          description: "Average {{ $value }} pending jobs per worker. Consider scaling up."

      # Alerta quando job esta demorando muito
      - alert: JobTakingTooLong
        expr: job_duration_seconds > 1800
        for: 0m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Job taking too long"
          description: "Job {{ $labels.job_id }} has been running for {{ $value | humanizeDuration }}"

      # Alerta quando taxa de falha de jobs esta alta
      - alert: HighJobFailureRate
        expr: |
          sum(rate(jobs_failed_total[1h]))
          / sum(rate(jobs_completed_total[1h]) + rate(jobs_failed_total[1h])) * 100 > 10
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High job failure rate"
          description: "{{ $value | humanize }}% of jobs are failing over the last hour"

  # ---------------------------------------------------------------------------
  # LLM Alerts - Alertas de consumo de tokens
  # ---------------------------------------------------------------------------
  - name: llm-alerts
    rules:
      # Alerta quando consumo de tokens esta alto
      - alert: HighTokenConsumption
        expr: increase(llm_tokens_total[1h]) > 100000
        for: 0m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High LLM token consumption"
          description: "{{ $value }} tokens used in the last hour. Monitor costs closely."

      # Alerta quando consumo de tokens critico
      - alert: CriticalTokenConsumption
        expr: increase(llm_tokens_total[1h]) > 500000
        for: 0m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Critical LLM token consumption"
          description: "{{ $value }} tokens used in the last hour! Potential runaway process."

  # ---------------------------------------------------------------------------
  # Infrastructure Alerts - Alertas de infraestrutura
  # ---------------------------------------------------------------------------
  - name: infrastructure-alerts
    rules:
      # Alerta quando Redis esta down
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Redis is down"
          description: "Redis on {{ $labels.instance }} is not responding"

      # Alerta quando PostgreSQL esta down
      - alert: PostgresDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL on {{ $labels.instance }} is not responding"

      # Alerta quando conexoes do postgres estao quase esgotadas
      - alert: PostgresConnectionsHigh
        expr: pg_stat_activity_count / pg_settings_max_connections * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "PostgreSQL connections high"
          description: "{{ $value | humanize }}% of PostgreSQL connections are in use"

      # Alerta quando memoria do Redis esta alta
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Redis memory usage high"
          description: "Redis is using {{ $value | humanize }}% of available memory"

      # Alerta quando disco esta cheio
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Disk space is low"
          description: "Only {{ $value | humanize }}% disk space remaining on {{ $labels.mountpoint }}"
